// SimpleOracleTNFMTest.scala
package com.siebel.oracle

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import org.apache.log4j.{Logger, Level}

object SimpleOracleTNFMTest {
  def main(args: Array[String]): Unit = {

    Logger.getLogger("org.apache.spark").setLevel(Level.WARN)
    Logger.getLogger("org.apache.hadoop").setLevel(Level.WARN)
    Logger.getLogger("com.siebel.oracle").setLevel(Level.INFO) // à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹€à¸›à¹‡à¸™ INFO à¸ªà¸³à¸«à¸£à¸±à¸š Production Logs
    Logger.getLogger("oracle.jdbc").setLevel(Level.INFO) // à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹€à¸›à¹‡à¸™ INFO à¸ªà¸³à¸«à¸£à¸±à¸š Production Logs
    // à¸–à¹‰à¸²à¸•à¹‰à¸­à¸‡à¸à¸²à¸£ debug à¹€à¸à¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡ à¸ªà¸²à¸¡à¸²à¸£à¸–à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹€à¸›à¹‡à¸™ Level.DEBUG/TRACE à¹„à¸”à¹‰
    Logger.getLogger("org.apache.spark.sql.execution.datasources.jdbc").setLevel(Level.DEBUG) // à¸ªà¸³à¸«à¸£à¸±à¸š JDBC Query Details

    val spark = SparkSession.builder()
      .appName("Scala Oracle Transform and Iceberg Test")
      // à¹€à¸à¸´à¹ˆà¸¡ Iceberg Catalog configuration à¸—à¸µà¹ˆà¸™à¸µà¹ˆ
      .config("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkSessionCatalog")
      .config("spark.sql.catalog.spark_catalog.type", "hadoop") // à¸«à¸£à¸·à¸­ "hive"
      .config("spark.sql.catalog.spark_catalog.warehouse", "gs://your-iceberg-warehouse-bucket/warehouse") // GCS bucket à¸ªà¸³à¸«à¸£à¸±à¸š Iceberg warehouse
      .getOrCreate()

    // Oracle RDS connection configuration (à¸£à¸±à¸šà¸ˆà¸²à¸ Environment Variables)
    val oracleUrl = sys.env("ORACLE_URL")
    val username = sys.env("ORACLE_USER")
    val password = sys.env("ORACLE_PASSWORD")

    println(s"DEBUG: Actual ORACLE_URL from env: $oracleUrl")
    println(s"DEBUG: Actual ORACLE_USER from env: $username")
    println(s"DEBUG: Actual ORACLE_PASSWORD from env: ${"*" * password.length}")

    try {
      // 1. à¸­à¹ˆà¸²à¸™ Data à¸ˆà¸²à¸ Oracle (Bronze Layer)
      println("\n--- Step 1: Ingesting Bronze Layer from Oracle ---")
      val bronzeDF = spark.read.format("jdbc")
        .option("url", oracleUrl)
        // à¸”à¸¶à¸‡à¸•à¸²à¸£à¸²à¸‡à¸ˆà¸£à¸´à¸‡à¸—à¸µà¹ˆà¸„à¸¸à¸“à¸•à¹‰à¸­à¸‡à¸à¸²à¸£ (à¹€à¸Šà¹ˆà¸™ EMPLOYEES)
        .option("dbtable", "EMPLOYEES") // à¸ªà¸¡à¸¡à¸•à¸´à¸§à¹ˆà¸²à¸¡à¸µà¸•à¸²à¸£à¸²à¸‡ EMPLOYEES
        .option("user", username)
        .option("password", password)
        .option("driver", "oracle.jdbc.driver.OracleDriver")
        .load()

      bronzeDF.printSchema()
      bronzeDF.show(5)
      println(s"Bronze Layer: Read ${bronzeDF.count()} records from EMPLOYEES.")

      // à¸ªà¸£à¹‰à¸²à¸‡ Temp View à¸ªà¸³à¸«à¸£à¸±à¸š Bronze Layer
      bronzeDF.createOrReplaceTempView("bronze_employees")
      println("--- Bronze Layer (bronze_employees) created as Temp View ---")

      // 2. Transform Data (Silver Layer)
      println("\n--- Step 2: Transforming to Silver Layer ---")
      val silverDF = spark.sql(
        """
          SELECT
            EMP_ID,
            NAME,
            DEPARTMENT,
            SALARY,
            SALARY * 1.05 as ANNUAL_SALARY_INCREASED, -- à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸à¸²à¸£ Transform
            current_timestamp() as load_timestamp
          FROM bronze_employees
          WHERE SALARY > 50000
        """
      )

      silverDF.printSchema()
      silverDF.show(5)
      println(s"Silver Layer: Transformed ${silverDF.count()} records.")

      // 3. Insert into Iceberg Table (Gold Layer à¸«à¸£à¸·à¸­ Staging Iceberg)
      println("\n--- Step 3: Inserting into Iceberg Table ---")
      // à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸² spark_catalog.your_iceberg_table à¸¡à¸µà¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§ à¸«à¸£à¸·à¸­à¸ˆà¸°à¸ªà¸£à¹‰à¸²à¸‡à¹ƒà¸«à¸¡à¹ˆ
      // à¹ƒà¸™ Dataproc Cluster à¸•à¹‰à¸­à¸‡à¹€à¸›à¸´à¸” Iceberg Component
      
      // à¸ªà¸£à¹‰à¸²à¸‡à¸•à¸²à¸£à¸²à¸‡ Iceberg à¸«à¸²à¸à¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¸¡à¸µ
      spark.sql(
        s"CREATE TABLE IF NOT EXISTS spark_catalog.employee_transformed (" +
        s"  EMP_ID BIGINT," +
        s"  NAME STRING," +
        s"  DEPARTMENT STRING," +
        s"  SALARY DOUBLE," +
        s"  ANNUAL_SALARY_INCREASED DOUBLE," +
        s"  load_timestamp TIMESTAMP" +
        s") USING iceberg"
      )
      println("Iceberg table 'employee_transformed' ensured exists.")

      // à¹€à¸‚à¸µà¸¢à¸™à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸¥à¸‡ Iceberg
      silverDF.writeTo("spark_catalog.employee_transformed").append() // à¸«à¸£à¸·à¸­ .overwrite() à¸«à¸²à¸à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¹€à¸‚à¸µà¸¢à¸™à¸—à¸±à¸š
      println("âœ… Data successfully written to Iceberg table 'employee_transformed'!")

      // (Optional) Query à¸ˆà¸²à¸ Iceberg à¹€à¸à¸·à¹ˆà¸­à¸¢à¸·à¸™à¸¢à¸±à¸™
      println("\n--- Verifying Data from Iceberg ---")
      val finalDF = spark.table("spark_catalog.employee_transformed")
      finalDF.show()
      println(s"Total records in Iceberg table: ${finalDF.count()}")

      println("\nğŸ‰ All Scala Oracle, Transform, and Iceberg tests completed successfully!")
      println("==========================================")

    } catch {
      case e: Exception =>
        println(s"âŒ An error occurred: ${e.getMessage}")
        e.printStackTrace()
        println("==========================================")
    } finally {
      spark.stop()
    }
  }
}